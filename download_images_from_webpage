import os
import hashlib

import requests
from bs4 import BeautifulSoup


page_url = '###'  #page url here

# Download page html
page_data = requests.get(page_url).text

# Find all links in page
images_urls = [
    image.attrs.get('src')
    for image in BeautifulSoup(page_data, 'lxml').find_all('img')
]

# Clean empty links (<img src="" /> <img> etc)
images_urls = [
    image_url
    for image_url in images_urls
    if image_url and len(image_url)>0
]

# Download files
def download_image(dest_dir):
    # TODO: add filename extension
    for image_url in images_urls:
        with open(os.path.join(dest_dir,image_url), 'wb') as f:
            source_url='http://www.burlingamepezmuseum.com/'+image_url
            image_data = requests.get(source_url).content
            f.write(image_data)





folder_name=page_url.replace(page_url,'')
newpath=r'C:\Users\muham\PycharmProjects\image_processing\\' +folder_name
if not os.path.exists(newpath):
    os.makedirs(newpath)

download_image(newpath)
